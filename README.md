# OpenShift vSphere Deployment Automation

Automated deployment scripts for installing Red Hat OpenShift on VMware vSphere with comprehensive static IP configuration, custom manifests, and robust cloud provider initialization handling.

## Features

- âœ… **Automated vSphere VM deployment** using govc
- âœ… **ğŸ†• Comprehensive static IP configuration** for all nodes (bootstrap, masters, workers)
- âœ… **ğŸ†• Individual ignition files** for each node with specific static IPs
- âœ… **Custom manifest injection** (vSphere credentials, console authentication, user passwords)
- âœ… **Shared RHCOS template** management
- âœ… **Dynamic network configuration** from cluster YAML
- âœ… **Load balancer integration** (HAProxy)
- âœ… **Cloud provider taint handling** for reliable bootstrap
- âœ… **Backup and debugging** capabilities
- âœ… **End-to-end deployment monitoring** with status reporting
- ğŸ†• **Robust vSphere credential management** with format validation
- ğŸ†• **Automatic credential error prevention** and format checking
- ğŸ†• **Per-node static IP assignment** via individual ignition files

## Prerequisites

### Software Requirements
- [OpenShift installer](https://mirror.openshift.com/pub/openshift-v4/clients/ocp/) (4.16.36+)
- [govc](https://github.com/vmware/govmomi/tree/master/govc) (VMware vSphere CLI)
- [yq](https://github.com/mikefarah/yq) (YAML processor)
- [jq](https://stedolan.github.io/jq/) (JSON processor)

### Infrastructure Requirements
- VMware vSphere environment (vCenter + ESXi)
- Load balancer (HAProxy recommended)
- DNS server with required entries
- RHCOS OVA template

## Quick Start

### 1. Clone and Setup
```bash
git clone <your-repo>
cd openshift-vsphere-automation
chmod +x scripts/*.sh
```

### 2. Configure Environment
```bash
# Copy and edit the govc environment file
cp govc.env.example govc.env
# Edit govc.env with your vSphere credentials
# Set GOVC_USERNAME (e.g., administrator@vsphere.sboyle.internal)
# Set GOVC_PASSWORD in environment or script will prompt
```

### 3. Prepare Assets
```bash
# Download RHCOS OVA
wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.16/rhcos-4.16.36-x86_64-vmware.x86_64.ova -O assets/

# Add your SSH public key
cp ~/.ssh/id_rsa.pub assets/ssh-key.pub

# Add your Red Hat pull secret
# Get from: https://console.redhat.com/openshift/install/pull-secret
cat > assets/pull-secret.json <<EOF
{"auths":{"your-pull-secret-here"}}
EOF

# Generate console password hash
python3 -c "import crypt; print(crypt.crypt('YourPassword', crypt.mksalt(crypt.METHOD_SHA512)))" > assets/console-password.txt
```

### 4. Configure Cluster
```bash
# Edit your cluster configuration
cp clusters/ocp416.yaml.example clusters/ocp416.yaml
# Customize cluster settings, network, DNS, etc.
```

### 5. Deploy
```bash
# Full automated deployment with static IPs and credential validation
./scripts/rebuild-cluster.sh clusters/ocp416.yaml
```

The deployment script now includes:
- **ğŸ†• Dynamic network configuration** - reads network settings from cluster YAML
- **ğŸ†• Individual node ignition files** - each VM gets specific static IP configuration
- **Automatic bootstrap monitoring** - waits for bootstrap completion
- **ğŸ†• vSphere credential validation** - ensures credentials work before deployment
- **ğŸ†• Credential format verification** - prevents authentication cascade failures
- **Cloud provider taint detection and removal** - prevents scheduling failures
- **Critical pod verification** - ensures etcd and cloud operators start properly
- **Installation completion monitoring** - waits for full cluster readiness
- **Comprehensive status reporting** - shows final cluster health

## Configuration

### Cluster Configuration (clusters/ocp416.yaml)
```yaml
clusterName: ocp416
baseDomain: openshift.sboyle.internal
vcenter_server: vcenter1.sboyle.internal
vcenter_username: administrator@vsphere.sboyle.internal
vcenter_datacenter: Lab
vcenter_cluster: Lab Cluster
vcenter_datastore: datastore-SAN1
vcenter_network: OpenShift_192.168.42.0
sshKeyFile: assets/ssh-key.pub
pullSecretFile: assets/pull-secret.json
consolePasswordFile: assets/console-password.txt
# ğŸ†• Dynamic network configuration
network:
  cidr: 192.168.42.0/24
  gateway: 192.168.42.1
  dns_servers:
    - 192.168.1.97
    - 192.168.1.98
```

### ğŸ†• Static IP Assignments (Automatic)
The deployment now automatically assigns static IPs based on your network configuration:

- **Bootstrap**: `{network_base}.30` (e.g., 192.168.42.30)
- **Master-0**: `{network_base}.31` (e.g., 192.168.42.31)
- **Master-1**: `{network_base}.32` (e.g., 192.168.42.32)
- **Master-2**: `{network_base}.33` (e.g., 192.168.42.33)
- **Worker-0**: `{network_base}.40` (e.g., 192.168.42.40)
- **Worker-1**: `{network_base}.41` (e.g., 192.168.42.41)

Where `{network_base}` is automatically extracted from your `network.cidr` configuration.

### Load Balancer Configuration
Required DNS entries and load balancer backends:
- `api.ocp416.openshift.sboyle.internal` â†’ 192.168.42.10 (HAProxy)
- `*.apps.ocp416.openshift.sboyle.internal` â†’ 192.168.42.20 (HAProxy)

**ğŸ†• Load Balancer Backend Configuration:**
```
# API Backend (port 6443)
server bootstrap 192.168.42.30:6443 check
server master-0 192.168.42.31:6443 check
server master-1 192.168.42.32:6443 check
server master-2 192.168.42.33:6443 check

# Apps Backend (ports 80/443)
server worker-0 192.168.42.40:80 check
server worker-1 192.168.42.41:80 check
```

See [docs/haproxy-config.md](docs/haproxy-config.md) for complete HAProxy setup.

## ğŸ†• Static IP Implementation

### How Static IPs Work
The deployment uses a multi-layered approach for reliable static IP assignment:

1. **Bootstrap Node**: Uses machine config manifest that gets embedded in bootstrap.ign
2. **Master Nodes**: Individual ignition files (master-0.ign, master-1.ign, master-2.ign) with specific IPs
3. **Worker Nodes**: Individual ignition files (worker-0.ign, worker-1.ign) with specific IPs

### ğŸ†• Individual Ignition Files
Each VM gets its own ignition file with embedded static IP configuration:

```bash
install-configs/ocp416/
â”œâ”€â”€ bootstrap.ign      # Contains static IP for .30
â”œâ”€â”€ master-0.ign       # Contains static IP for .31
â”œâ”€â”€ master-1.ign       # Contains static IP for .32
â”œâ”€â”€ master-2.ign       # Contains static IP for .33
â”œâ”€â”€ worker-0.ign       # Contains static IP for .40
â”œâ”€â”€ worker-1.ign       # Contains static IP for .41
â””â”€â”€ worker.ign         # Original (unused)
```

### ğŸ†• NetworkManager Configuration
Each ignition file contains a NetworkManager connection profile:

```ini
[connection]
id=ens192
type=ethernet
interface-name=ens192
autoconnect=true
autoconnect-priority=999

[ethernet]

[ipv4]
address1=192.168.42.31/24,192.168.42.1
dns=192.168.1.97;
method=manual

[ipv6]
addr-gen-mode=eui64
method=disabled
```

### ğŸ†• Dynamic Network Parsing
The scripts automatically read network configuration from your cluster YAML:

```yaml
# Supported formats:
network:
  cidr: 192.168.42.0/24
  gateway: 192.168.42.1
  dns_servers:
    - 192.168.1.97
    - 192.168.1.98

# Or simplified format:
network: 192.168.42.0/24

# Or alternative naming:
subnet:
  cidr: 10.0.100.0/24
  gateway: 10.0.100.1
```

## Scripts Overview

| Script | Purpose |
|--------|---------|
| `rebuild-cluster.sh` | **Main orchestration script** - full cluster rebuild with static IPs and cloud provider handling |
| `generate-static-ip-manifests.sh` | **ğŸ†• Enhanced** - Create NetworkManager static IP configs with dynamic network parsing |
| `create-individual-node-ignitions.sh` | **ğŸ†• NEW** - Generate individual ignition files for each node with specific static IPs |
| `deploy-vms.sh` | **ğŸ†• Enhanced** - Deploy VMs using individual ignition files |
| `fix-cloud-provider-taints.sh` | **NEW** - Detects and fixes cloud provider initialization issues |
| `validate-credentials.sh` | **ğŸ†• NEW** - Validates vSphere credentials and secret format |
| `delete-cluster.sh` | Clean up VMs and generated configs |
| `deploy-cluster.sh` | **ğŸ†• Enhanced** - Generate manifests with proper credential format |
| `generate-install-config.sh` | **ğŸ†• Fixed** - Create install-config.yaml with real passwords (no placeholders) |
| `generate-core-password-manifest.sh` | Set console access password for core user |
| `generate-vsphere-creds-manifest.sh` | **ğŸ†• Fixed** - Inject vSphere credentials with standard format |
| `generate-console-password-manifests.sh` | Set up OpenShift console authentication |
| `inject-static-ips-into-ignition.sh` | Direct ignition file modification for static IPs |
| `load-vcenter-env.sh` | **ğŸ†• Enhanced** - Load and validate vSphere environment variables |

## ğŸ†• Enhanced Static IP Management

### Automatic Network Configuration
The deployment scripts now automatically:
- **Parse network settings** from cluster YAML configuration
- **Extract network base** from CIDR notation (e.g., 192.168.42.0/24 â†’ 192.168.42)
- **Use provided gateway** or default to `.1`
- **Use provided DNS servers** or fall back to gateway
- **Generate sequential IP assignments** based on node type and index

### Individual Node Configurations
Each node gets a tailored configuration:

```bash
# Bootstrap node - via machine config in bootstrap.ign
Bootstrap: {network_base}.30

# Master nodes - via individual ignition files
Master-0:  {network_base}.31  (master-0.ign)
Master-1:  {network_base}.32  (master-1.ign)
Master-2:  {network_base}.33  (master-2.ign)

# Worker nodes - via individual ignition files
Worker-0:  {network_base}.40  (worker-0.ign)
Worker-1:  {network_base}.41  (worker-1.ign)
```

### ğŸ†• Deployment Integration
The `deploy-vms.sh` script automatically detects and uses the correct ignition file:

```bash
# VM naming to ignition file mapping:
ocp416-bootstrap â†’ bootstrap.ign
ocp416-master-0  â†’ master-0.ign
ocp416-master-1  â†’ master-1.ign
ocp416-master-2  â†’ master-2.ign
ocp416-worker-0  â†’ worker-0.ign
ocp416-worker-1  â†’ worker-1.ign
```

## ğŸ†• Credential Management Enhancements

### What Was Fixed
The deployment automation now includes robust credential handling that prevents the most common vSphere authentication failures:

**Previous Issues:**
- âŒ Placeholder passwords (`WILL_BE_SET_BY_ENVIRONMENT`) remained in secrets
- âŒ Server-specific credential keys (`vcenter.domain.com.username`) instead of standard format
- âŒ Missing credential validation before deployment
- âŒ Authentication cascade failures affecting entire cluster

**New Solutions:**
- âœ… **Real passwords embedded** in install-config.yaml and manifests
- âœ… **Standard credential format** (`username`/`password` keys) used consistently
- âœ… **Pre-deployment credential validation** ensures vSphere connectivity
- âœ… **Format verification** prevents authentication failures
- âœ… **Comprehensive secret generation** for all required components

### Credential Validation
```bash
# Validate credentials before deployment
./scripts/validate-credentials.sh ocp416

# Validate deployed cluster credentials
export KUBECONFIG=install-configs/ocp416/auth/kubeconfig
./scripts/validate-credentials.sh ocp416
```

### Credential Format
The scripts now ensure all vSphere secrets use the **standard format**:
```yaml
data:
  username: <base64-encoded-username>
  password: <base64-encoded-password>
```

**Not the problematic server-specific format:**
```yaml
data:
  vcenter1.sboyle.internal.username: <base64>
  vcenter1.sboyle.internal.password: <base64>
```

## Deployment Flow

The enhanced `rebuild-cluster.sh` now follows this robust deployment flow:

1. **ğŸ†• Credential Validation**: Verify vSphere credentials and format
2. **Pre-deployment**: Clean up, generate configs, inject manifests
3. **ğŸ†• Static IP Manifest Generation**: Create NetworkManager configs with dynamic network parsing
4. **ğŸ†• Credential Format Check**: Ensure no placeholders remain
5. **Ignition Generation**: Create base ignition files
6. **ğŸ†• Individual Node Ignition Creation**: Generate specific ignition files for each node with static IPs
7. **VM Deployment**: Deploy and configure VMs with individual ignition files
8. **Bootstrap Monitoring**: Wait for bootstrap completion (up to 40 minutes)
9. **Cloud Provider Handling**: Automatically detect and fix cloud provider initialization issues
10. **Critical Pod Verification**: Ensure etcd-operator and cloud-credential-operator are running
11. **Installation Completion**: Wait for full cluster installation
12. **ğŸ†• Final Credential Validation**: Verify deployed credentials work correctly
13. **ğŸ†• Static IP Verification**: Confirm all nodes have their assigned static IPs
14. **Status Reporting**: Show final cluster health and any issues

### ğŸ†• Static IP Verification

The deployment process now includes automatic verification:

```bash
# Automatically checks that each VM has its expected IP
âœ… Static IP verification:
   Bootstrap (192.168.42.30): âœ… Configured
   Master-0 (192.168.42.31):  âœ… Configured  
   Master-1 (192.168.42.32):  âœ… Configured
   Master-2 (192.168.42.33):  âœ… Configured
   Worker-0 (192.168.42.40):  âœ… Configured
   Worker-1 (192.168.42.41):  âœ… Configured
```

### ğŸ†• Credential Error Prevention

The new credential handling automatically:
- **Validates username format** (should be user@domain.tld)
- **Tests vSphere connectivity** before VM deployment
- **Removes placeholder passwords** from all manifests
- **Uses consistent secret format** across all components
- **Verifies credential secrets** in deployed cluster

This prevents the common authentication cascade where:
```
Machine API can't authenticate â†’ Control plane machines fail â†’ 
Authentication operator fails â†’ Console fails â†’ Ingress fails
```

### Cloud Provider Taint Handling

The `fix-cloud-provider-taints.sh` script automatically:
- Detects `node.cloudprovider.kubernetes.io/uninitialized` taints that prevent pod scheduling
- Removes these taints when cloud provider initialization is delayed
- Verifies critical system pods can schedule and start
- Provides detailed logging for troubleshooting

## Usage Examples

### Deploy New Cluster (Enhanced with Static IP and Credential Validation)
```bash
# Full deployment with automatic static IP assignment and credential validation
./scripts/rebuild-cluster.sh clusters/ocp416.yaml
```

The script will now show enhanced progress like:
```
ğŸ” Validating vSphere credentials...
âœ… vSphere connectivity confirmed
ğŸ“¡ Using network: 192.168.42.0/24
ğŸ  Network base: 192.168.42
ğŸšª Gateway: 192.168.42.1
ğŸŒ DNS: 192.168.1.97
ğŸŒ Generating static IP manifests for cluster ocp416...
âœ… Static IP manifests generated:
   Bootstrap: 192.168.42.30
   Master-0:  192.168.42.31
   Master-1:  192.168.42.32
   Master-2:  192.168.42.33
   Worker-0:  192.168.42.40
   Worker-1:  192.168.42.41
ğŸ”§ Creating individual master and worker ignition files with static IPs...
âœ… Individual ignition files created
ğŸ“‹ Loaded credentials for: administrator@vsphere.sboyle.internal @ vcenter1.sboyle.internal
ğŸ” Creating ALL required vSphere credentials secrets...
âœ… Username format looks correct (contains @ and domain)
ğŸš€ Deploying VMs...
âœ… Applied ignition config: master-0.ign
âœ… Applied ignition config: master-1.ign
âœ… Applied ignition config: master-2.ign
âœ… Applied ignition config: worker-0.ign
âœ… Applied ignition config: worker-1.ign
ğŸ‰ VM deployment complete!
â³ Waiting for cluster bootstrap to complete...
âœ… Bootstrap completed successfully
ğŸ”§ Checking and fixing cloud provider initialization issues...
ğŸ” Validating deployed credentials...
âœ… All credential validations passed
âœ… Static IP verification completed
ğŸ Final cluster status:
âœ… Cluster API is accessible
âœ… All cluster operators healthy
```

### ğŸ†• Generate Static IP Configurations Only
```bash
# Generate static IP manifests without full deployment
./scripts/generate-static-ip-manifests.sh clusters/ocp416.yaml

# Generate individual ignition files with static IPs
./scripts/create-individual-node-ignitions.sh clusters/ocp416.yaml
```

### ğŸ†• Validate Credentials (New Feature)
```bash
# Validate credentials before deployment
./scripts/validate-credentials.sh ocp416

# Validate deployed cluster credentials
export KUBECONFIG=install-configs/ocp416/auth/kubeconfig
./scripts/validate-credentials.sh ocp416
```

### ğŸ†• Fix Credential Issues (Manual)
```bash
# If you encounter credential format issues in existing cluster
oc delete secret vsphere-cloud-credentials -n openshift-machine-api
oc create secret generic vsphere-cloud-credentials \
  --from-literal=username="administrator@vsphere.sboyle.internal" \
  --from-literal=password="your-password" \
  -n openshift-machine-api
oc delete pods -n openshift-machine-api -l k8s-app=machine-api-controllers
```

### Fix Cloud Provider Issues (Manual)
```bash
# If you need to manually fix cloud provider issues on existing cluster
./scripts/fix-cloud-provider-taints.sh install-configs/ocp416
```

### Delete Cluster (with confirmation)
```bash
./scripts/delete-cluster.sh clusters/ocp416.yaml
```

## Monitoring Installation

### Automated Monitoring (Built-in)
The `rebuild-cluster.sh` script now includes comprehensive monitoring:
- **ğŸ†• Network configuration parsing and validation**
- **ğŸ†• Static IP assignment verification**
- **ğŸ†• Individual ignition file creation and validation**
- **ğŸ†• Credential validation** at multiple stages
- Bootstrap completion detection
- Cloud provider issue detection and fixing
- Critical pod readiness verification
- Installation completion monitoring
- **ğŸ†• Final credential verification**
- **ğŸ†• Final static IP verification**
- Final cluster status reporting

### Manual Monitoring
```bash
cd install-configs/ocp416

# Watch bootstrap progress
openshift-install wait-for bootstrap-complete --log-level debug

# Watch installation completion
openshift-install wait-for install-complete --log-level debug

# Check cluster status
export KUBECONFIG=auth/kubeconfig
oc get nodes
oc get co  # Check cluster operators
oc get pods --all-namespaces | grep -v Running

# ğŸ†• Check node IP assignments
oc get nodes -o wide

# ğŸ†• Verify static IP configurations
for vm in bootstrap master-0 master-1 master-2 worker-0 worker-1; do
  echo "Checking ${vm}..."
  govc vm.ip ocp416-${vm}
done

# ğŸ†• Check machine authentication status
oc describe machines -n openshift-machine-api | grep -A5 -B5 "Cannot complete login"
```

## Console Access

### Web Console
- **URL**: `https://console-openshift-console.apps.ocp416.openshift.sboyle.internal`
- **Admin User**: `admin` / `[your-password]`
- **Kubeadmin**: `kubeadmin` / `[generated-password]`

### CLI Access
```bash
export KUBECONFIG=install-configs/ocp416/auth/kubeconfig
oc login -u admin
```

### Node Console Access
```bash
# SSH with key
ssh core@192.168.42.30  # Bootstrap
ssh core@192.168.42.31  # Master-0
ssh core@192.168.42.32  # Master-1
ssh core@192.168.42.33  # Master-2
ssh core@192.168.42.40  # Worker-0
ssh core@192.168.42.41  # Worker-1

# vCenter console with password
# Username: core
# Password: [your-password]
```

## Troubleshooting

### ğŸ†• Automatic Static IP Issue Resolution

The deployment scripts now automatically handle:
- **ğŸ†• Dynamic network configuration parsing** - reads from cluster YAML
- **ğŸ†• Individual ignition file generation** - ensures each node gets specific IP
- **ğŸ†• NetworkManager configuration validation** - verifies network configs are correct
- **ğŸ†• IP assignment verification** - confirms VMs get their expected IPs
- **ğŸ†• Credential format validation** - ensures standard format used
- **ğŸ†• Placeholder password removal** - prevents authentication failures
- **ğŸ†• vSphere connectivity testing** - validates credentials before deployment
- **ğŸ†• Machine API authentication** - verifies credentials work in deployed cluster
- **Cloud provider initialization delays** - automatically removes blocking taints
- **Pod scheduling failures** - verifies critical pods can start
- **Bootstrap timeouts** - continues with installation after fixing issues

### Common Issues

**ğŸ†• Static IP Assignment Issues (Automatically Prevented):**
- **Symptoms**: VMs not getting expected static IPs
- **Automatic Prevention**: Individual ignition files ensure each VM gets specific IP
- **Manual Check**: `govc vm.ip ocp416-master-0` should show 192.168.42.31
- **Manual Fix**: Check NetworkManager logs: `sudo journalctl -u NetworkManager`
- **Root Cause**: Usually network configuration or DNS issues

**ğŸ†• Network Configuration Issues (Automatically Detected):**
- **Symptoms**: Script fails to parse network from cluster YAML
- **Automatic Detection**: Script validates network configuration before proceeding
- **Manual Fix**: Ensure cluster YAML has valid `network.cidr` or `subnet.cidr`
- **Example**: `network: { cidr: "192.168.42.0/24", gateway: "192.168.42.1" }`

**ğŸ†• Individual Ignition File Issues (Automatically Fixed):**
- **Symptoms**: VMs boot but don't get static IPs
- **Automatic Fix**: Script validates ignition files contain NetworkManager configs
- **Manual Check**: `jq '.storage.files[] | select(.path | contains("system-connections"))' master-0.ign`
- **Manual Fix**: Re-run `./scripts/create-individual-node-ignitions.sh clusters/ocp416.yaml`

**ğŸ†• vSphere Authentication Failures (Automatically Prevented):**
- **Symptoms**: Machine API cannot connect to vSphere, authentication errors
- **Automatic Prevention**: Scripts validate credentials and format before deployment
- **Manual Fix**: Run `./scripts/validate-credentials.sh` to diagnose and fix
- **Root Cause**: Usually placeholder passwords or server-specific credential keys

**ğŸ†• Credential Format Issues (Automatically Fixed):**
- **Symptoms**: Secrets exist but Machine API can't read them
- **Automatic Fix**: Scripts generate all secrets with standard format
- **Manual Check**: `oc get secret vsphere-cloud-credentials -n openshift-machine-api -o yaml`
- **Should see**: `username:` and `password:` keys, not server-specific keys

**Cloud Provider Initialization Issues (Automatically Fixed):**
- **Symptoms**: Pods stuck in "Pending" with `node.cloudprovider.kubernetes.io/uninitialized` taints
- **Automatic Fix**: The script detects and removes these taints
- **Manual Fix**: Run `./scripts/fix-cloud-provider-taints.sh install-configs/ocp416`

**VMs not getting static IPs:**
- Check manifest backup in `install-configs/ocp416/manifests-backup-*/`
- Verify DNS servers are reachable
- Check NetworkManager logs: `sudo journalctl -u NetworkManager`
- Verify individual ignition files exist: `ls install-configs/ocp416/master-*.ign worker-*.ign`

**Bootstrap timeout:**
- The script now handles this automatically by proceeding to cloud provider checks
- Check HAProxy configuration for port 22623
- Verify masters can reach bootstrap: `curl -k https://192.168.42.30:22623/config/master`

### ğŸ†• Enhanced Debug Commands
```bash
# Validate network configuration parsing
./scripts/generate-static-ip-manifests.sh clusters/ocp416.yaml

# Check individual ignition files
ls -la install-configs/ocp416/*.ign
jq '.storage.files[] | select(.path | contains("system-connections")) | .path' install-configs/ocp416/master-0.ign

# Validate credentials in deployed cluster
./scripts/validate-credentials.sh ocp416

# Check credential format in secrets
oc get secret vsphere-cloud-credentials -n openshift-machine-api -o yaml

# Check for authentication errors in machines
oc describe machines -n openshift-machine-api | grep -A5 -B5 "Cannot complete login"

# Check VM IPs match expected assignments
echo "Expected vs Actual IP assignments:"
echo "Bootstrap (expected 192.168.42.30): $(govc vm.ip ocp416-bootstrap)"
echo "Master-0 (expected 192.168.42.31):  $(govc vm.ip ocp416-master-0)"
echo "Master-1 (expected 192.168.42.32):  $(govc vm.ip ocp416-master-1)"
echo "Master-2 (expected 192.168.42.33):  $(govc vm.ip ocp416-master-2)"
echo "Worker-0 (expected 192.168.42.40):  $(govc vm.ip ocp416-worker-0)"
echo "Worker-1 (expected 192.168.42.41):  $(govc vm.ip ocp416-worker-1)"

# Check cluster status
oc get nodes -o wide
oc get csr
oc get co

# Check for taint issues
oc get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# Check critical pods
oc get pods -n openshift-etcd-operator
oc get pods -n openshift-cloud-credential-operator
oc get pods -n openshift-machine-api

# Test vSphere connectivity
source scripts/load-vcenter-env.sh
govc about

# ğŸ†• Verify NetworkManager configurations on nodes
ssh core@192.168.42.31 "sudo cat /etc/NetworkManager/system-connections/ens192.nmconnection"
```

### Script Debugging

The enhanced scripts provide detailed logging:
```bash
# Check recent deployment logs
tail -f /tmp/openshift-install-*.log

# Check cloud provider fix logs
./scripts/fix-cloud-provider-taints.sh install-configs/ocp416

# ğŸ†• Check static IP generation logs
./scripts/generate-static-ip-manifests.sh clusters/ocp416.yaml

# ğŸ†• Check individual ignition creation logs
./scripts/create-individual-node-ignitions.sh clusters/ocp416.yaml

# ğŸ†• Check credential validation logs
./scripts/validate-credentials.sh ocp416
```

## File Structure
```
.
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ ssh-key.pub
â”‚   â”œâ”€â”€ pull-secret.json
â”‚   â”œâ”€â”€ console-password.txt
â”‚   â””â”€â”€ rhcos-4.16.36-x86_64-vmware.x86_64.ova
â”œâ”€â”€ clusters/
â”‚   â””â”€â”€ ocp416.yaml                      # ğŸ†• Enhanced with network config
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ rebuild-cluster.sh               # Enhanced with static IP orchestration
â”‚   â”œâ”€â”€ generate-static-ip-manifests.sh  # ğŸ†• Enhanced with dynamic network parsing
â”‚   â”œâ”€â”€ create-individual-node-ignitions.sh # ğŸ†• NEW - Individual ignition files
â”‚   â”œâ”€â”€ deploy-vms.sh                    # ğŸ†• Enhanced to use individual ignitions
â”‚   â”œâ”€â”€ fix-cloud-provider-taints.sh     # Cloud provider issue resolution
â”‚   â”œâ”€â”€ validate-credentials.sh          # ğŸ†• NEW - Credential validation
â”‚   â”œâ”€â”€ delete-cluster.sh
â”‚   â”œâ”€â”€ deploy-cluster.sh                # ğŸ†• Enhanced credential generation
â”‚   â”œâ”€â”€ generate-install-config.sh       # ğŸ†• Fixed placeholder password issue
â”‚   â”œâ”€â”€ load-vcenter-env.sh              # ğŸ†• Enhanced validation
â”‚   â””â”€â”€ [other scripts...]
â”œâ”€â”€ install-configs/
â”‚   â””â”€â”€ ocp416/
â”‚       â”œâ”€â”€ bootstrap.ign               # Contains static IP for bootstrap
â”‚       â”œâ”€â”€ master-0.ign                # ğŸ†• Individual master ignitions
â”‚       â”œâ”€â”€ master-1.ign                # ğŸ†• with specific static IPs
â”‚       â”œâ”€â”€ master-2.ign                # ğŸ†•
â”‚       â”œâ”€â”€ worker-0.ign                # ğŸ†• Individual worker ignitions  
â”‚       â”œâ”€â”€ worker-1.ign                # ğŸ†• with specific static IPs
â”‚       â”œâ”€â”€ master.ign                  # Original (base template)
â”‚       â”œâ”€â”€ worker.ign                  # Original (base template)
â”‚       â”œâ”€â”€ auth/
â”‚       â””â”€â”€ manifests-backup-*/
â”œâ”€â”€ govc.env
â””â”€â”€ README.md
```

## What's New in v4.0

### ğŸ†• Comprehensive Static IP Management
- **Dynamic network configuration parsing** from cluster YAML
- **Individual ignition files** for each node with specific static IPs
- **Automatic IP assignment** based on node type and network configuration
- **NetworkManager integration** for reliable static IP configuration
- **Per-node IP verification** during deployment

### ğŸ†• Enhanced Network Configuration
- **Flexible YAML